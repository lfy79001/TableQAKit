<div align="center">
  <img src="figs/TableQAKit.png" border="0" width="512"/>
  <br />
  <br />


[ğŸŒWebsite](http://210.75.240.136:18888) |
[ğŸ“¦PyPI](https://www.baidu.com/)
[ğŸ¤—Huggingface Datasets](https://huggingface.co/TableQAKit)

<!-- [ğŸ“˜Documentation](https://opencompass.readthedocs.io/en/latest/) |
[ğŸ› ï¸Installation](https://opencompass.readthedocs.io/en/latest/get_started.html#installation) | -->


</div>

# TableQAKit: A Toolkit for Table Question Answering

# ğŸ”¥ Updates

- [**2023-8-7**]: We released our [code](https://github.com/lfy79001/TableQAKit) and [PyPI](https://www.baidu.com). Check it out!

# âœ¨ Features
TableQAKit is a unified platform for TableQA (especially in the LLM era). Its main features includes:
- **Extensible disign**: You can use the interfaces defined by the toolkit, extend methods and models, and implement your own new models based on your own data.
- **Equipped with LLM**: TableQAKit supports LLM-based methods, including LLM-prompting methods and LLM-finetuning methods.
- **Comprehensive datasets**: We design a unified data interface to process data and store them in Huggingface datasets.
- **Powerful methods**: Using our toolkit, you can reproduce most of the SOTA methods for TableQA tasks.
- **Efficient LLM benchmark**: TableQAEval, a benchmark to evaluate the performance of LLM for TableQA. It evaluates LLM's modeling ability of long tables (context) and comprehension capabilities (numerical reasoning, multi-hop reasoning).
- **Comprehensive Survey**: We are about to release a systematic TableQA Survey, this project is a pre-work.



# ğŸ”§ How to use

## Retrieval-then-Read Methods

### Retriever

#### QuickStart

MultiHiertt Dataset as a demonstration
```
from typing import Dict, List
import json
from retriever import MultiHierttTrainer


trainer = MultiHierttTrainer()
trainer.train()


args = get_training_args()
trainer = Trainer(training_args=args)
if args.train_path is not None:
    trainer.train()
if args.test_path is not None:
    for pred in trainer.test_iterator():
        # saving the predctions
```

#### Train
```
python main.py \
--train_mode row \
--per_device_train_batch_size 16 \
--per_device_eval_batch_size 1 \
--dataloader_pin_memory False \
--output_dir ./ckpt \
--train_path ./data/train.json \
--val_path ./data/val.json \
--save_steps 1000 \
--logging_steps 20 \
--learning_rate 0.00001 \
--top_n_for_eval 10 \
```

#### Inference
```
python infer.py \
--train_mode row \
--dataloader_pin_memory False \
--output_dir ./ckpt \
--test_path ./data/test-dev_out.json \
--ckpt_for_test ./ckpt/epoch3_step53000.pt \
--top_n_for_test 10 \
--encoder_path bert-base-uncased/
```

### Create Trainer for New Dataset
```
from retriever import RetrieverTrainer as RT

class NewTrainer(RT):
    def read_data(self, data_path: str) -> List[Dict]:
        """

        :param data_path: The path of data
        :return: List of raw data
        [
            data_1,
            data_2,
            â€¦â€¦
        ]
        """
        data = json.load(
            open(data_path, 'r', encoding='utf-8')
        )
        return data

    def data_proc(self, instance) -> Dict:
        """

        :return:
        {
            "id": str,
            "question": str,
            "rows": list[str],
            "labels": list[int]
        }
        """
        rows = instance["paragraphs"]
        labels = [0] * len(instance["paragraphs"])
        if len(instance["qa"]["text_evidence"]):
            for text_evidence in instance["qa"]["text_evidence"]:
                labels[text_evidence] = 1
        for k, v in instance["table_description"].items():
            rows.append(v)
            labels.append(1 if k in instance["qa"]["table_evidence"] else 0)
        return {
            "id": instance["uid"],
            "question": instance["qa"]["question"],
            "rows": rows,
            "labels": labels
        }
```

### TablePTM Reasoner



## LLM-Finetuning Methods





## LLM-Prompting Methods






# ğŸ—ƒï¸ Dataset
According to our taxonomy, we classify the TableQA task into three categories of tasks, as shown in the following figure:

<p align="center">
<img src="figs/dataset_examples.png" width="512">
</p>


<p align="center">
<img src="figs/table.png" width="512">
</p>




# Folder
The TableQAKit repository is structured as follows:

```bash
â”œâ”€â”€ Baselines/ # scripts to generate the prediction files with baseline models
â”œâ”€â”€ Evaluation/ # evaluation scripts
â”œâ”€â”€ Leaderboard/ # csv files of results
â”œâ”€â”€ LEval-data/ # test samples
â”‚   â”œâ”€â”€ Exam/ # exact match tasks (like multiple-choice)
â”‚   â”‚   â”œâ”€â”€ test_file.jsonl 
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Generation/ # generation tasks
â”‚   â”‚   â”œâ”€â”€ test_file.jsonl
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ Predictions/ # output of models
â”‚   â”œâ”€â”€ exam_eval/trubo-16k-0613
â”‚   â”‚              â”œâ”€â”€ <task_name>.pred.jsonl
â”‚   â”‚              â””â”€â”€ ... 
â”‚   â”œâ”€â”€ llm_gpt4_eval  
â”‚   â”‚             â”œâ”€â”€<model_name>.pred.jsonl
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ Tools/ # useful scripts
â”œâ”€â”€ figs/ # figures
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```



# TableQAEval

<p align="center">
<img src="figs/TableQAEval.png" width="400">
</p>

TableQAEval is a benchmark to evaluate the performance of LLM for TableQA. It evaluates LLM's modeling ability of long tables (context) and comprehension capabilities (numerical reasoning, multi-hop reasoning).

## Leaderboard

| Model | Parameters | Numerical Reasoning | Multi-hop Reasoning | Structured Reasoning | Total |
| ---   | ---        | ---                 | ---                 | ---                  | ---   |
| Turbo-16k-0613 | -     | 32.3               | 40.6 | 43.5 | 38.3 |
| longchat-7b-16k | 7B | 22.3 | 24.1 | 20.9 | 23.0 |
| xgen-7b-8k | 7B | 19.5 | 20.4 | 18.7 | 19.6  |
| mpt-7b-65k | 7B | 17.3 | 16.2 | 18.0 | 17.3 |
| LLaMA2-7b-chat | 7B | 16.2 | 18.4 | 20.1 | 18.6 |
| LLaMA2-7b-4k | 7B | 11.5 | 14.2 | 16.3 | 13.5 |
| LLaMA-7b-2k | 7B | 5.6 | 7.3 | 6.1 | 6.5 |





## QuickStart
```
pip install gunicorn

# è¿è¡Œåœ¨210.75.240.136:18889,è®¿é—®ï¼šhttp://210.75.240.136:18889
gunicorn -c gunicorn_config.py app:app --daemon

# æƒ³è¦åœæ­¢è¿è¡Œï¼Ÿ
åœ¨gunicorn_error.logæ‰¾åˆ°æœ€æ–°çš„è®°å½•Listeningè®°å½•ï¼Œå¦‚"Listening at: http://210.75.240.136:18889 (2609966)"
ä½¿ç”¨ kill 2609966 å¯å®ç°åœæ­¢è¿è¡Œ

```






## å¦‚ä½•å¾€PyPIä¸Šæäº¤
0. å®‰è£…å¿…è¦çš„å·¥å…·
   ```bash
   pip install setuptools wheel twine
1. å…ˆä¿®æ”¹setup.pyè¿™ä¸ªæ–‡ä»¶
2. ç”Ÿæˆåˆ†å‘æ–‡ä»¶ã€‚åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥ç”Ÿæˆæºä»£ç å‹ç¼©åŒ…å’Œè½®å­ï¼ˆwheelï¼‰åˆ†å‘æ–‡ä»¶ï¼š
   ```bash
    python setup.py sdist bdist_wheel
3. ä½¿ç”¨twineä¸Šä¼ åˆ°æµ‹è¯• PyPIã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥å°†æ‚¨çš„åˆ†å‘æ–‡ä»¶ä¸Šä¼ åˆ°æµ‹è¯• PyPIï¼š
    ```bash 
    twine upload --repository-url https://test.pypi.org/legacy/dist/*
    ```
    UserName: lfy79001

    PassWord: 20010213lfyLFY!
4. è®¿é—® https://test.pypi.org/project/ttqakit
5. ä¸Šä¼ åˆ°æ­£å¼çš„
    ```bash
    twine upload dist/*
    ```
    è®¿é—® https://pypi.org/project/ttqakit
6. å®Œæ¯•åå¯ä»¥å®‰è£…
    ```bash
    # æ­£å¼ç‰ˆ
    pip install ttqakit
    # testç‰ˆ
    pip install --index-url https://test.pypi.org/simple/ttqakit









